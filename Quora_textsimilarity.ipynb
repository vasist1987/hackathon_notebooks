{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Quora_textsimilarity.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1Lpmd__YdduUjCknX5VNRKsUUq4C2eR6e",
      "authorship_tag": "ABX9TyNZbPJ40luO/rOLaK3xiSYy",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "8b6ce4e4df444b0b9547c28303423f62": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_4085095406654de694e30e9f5160e275",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_c6227c6f24a348a3bacedbe54c4ec5a8",
              "IPY_MODEL_6232ee2c52a44a39884ca187c1623b00"
            ]
          }
        },
        "4085095406654de694e30e9f5160e275": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c6227c6f24a348a3bacedbe54c4ec5a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_0cfbae2575ba4af991d2513f57309efe",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 231508,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 231508,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_544c022dcfe941669d22634868fc6c7e"
          }
        },
        "6232ee2c52a44a39884ca187c1623b00": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_9875233a7cca478599e1fec463d9e3ad",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 232k/232k [00:00&lt;00:00, 3.09MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_fc39b58307584e04b608585a2dd9bf94"
          }
        },
        "0cfbae2575ba4af991d2513f57309efe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "544c022dcfe941669d22634868fc6c7e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9875233a7cca478599e1fec463d9e3ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "fc39b58307584e04b608585a2dd9bf94": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7fe3007c909a48319f42c905b5a22cba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_ad6abdc74c574edd8d8cae9aea3d2a43",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_655edcd3b9e041b78957c0a8b6ba619d",
              "IPY_MODEL_69f0084e1dc14456a137b9a8b0cc5715"
            ]
          }
        },
        "ad6abdc74c574edd8d8cae9aea3d2a43": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "655edcd3b9e041b78957c0a8b6ba619d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_983ac755dd774d70982e655363f2fc57",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 442,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 442,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_89ab82506ee24a3090324e2689a5c0bd"
          }
        },
        "69f0084e1dc14456a137b9a8b0cc5715": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_5a6fd32df45644358e5abb51f0f32b11",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 442/442 [00:00&lt;00:00, 2.40kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f5955935bfab48f4a7485ae103b0c678"
          }
        },
        "983ac755dd774d70982e655363f2fc57": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "89ab82506ee24a3090324e2689a5c0bd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5a6fd32df45644358e5abb51f0f32b11": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f5955935bfab48f4a7485ae103b0c678": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6a76d12b5c7742bea23e65957de743df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_893a5f08d6cb4942910bf9dbd2e0cbc0",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_2f8ebf45931b4df5a37637f895810afb",
              "IPY_MODEL_9156b82b81b74e86b3a9b2d5a4659b16"
            ]
          }
        },
        "893a5f08d6cb4942910bf9dbd2e0cbc0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2f8ebf45931b4df5a37637f895810afb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_ba2e15ba7e7e4cb8b71172bf1a4bbbe9",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 363423424,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 363423424,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d8b0e76a93e3499fb10562492635d99a"
          }
        },
        "9156b82b81b74e86b3a9b2d5a4659b16": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_69602240370b45d4b4beb733391111a2",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 363M/363M [00:11&lt;00:00, 30.4MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b5670ce131c241d29e4b6074c18b7284"
          }
        },
        "ba2e15ba7e7e4cb8b71172bf1a4bbbe9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d8b0e76a93e3499fb10562492635d99a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "69602240370b45d4b4beb733391111a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b5670ce131c241d29e4b6074c18b7284": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vasist1987/HackerRank_Python/blob/master/Quora_textsimilarity.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LeNHqzcr9ZHo",
        "outputId": "6712b486-f889-4087-ea64-5f2c4eb19c8f"
      },
      "source": [
        "\r\n",
        "import numpy as np # linear algebra\r\n",
        "from numpy import sqrt,argmax\r\n",
        "from time import time\r\n",
        "import pandas as pd  # data processing, CSV file I/O (e.g. pd.read_csv)\r\n",
        "import zipfile\r\n",
        "import pylab as pl\r\n",
        "#import dask.dataframe as dd\r\n",
        "from dask.multiprocessing import get\r\n",
        "import seaborn as sns\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import nltk\r\n",
        "from sklearn.ensemble import StackingClassifier\r\n",
        "import keras\r\n",
        "from tensorflow import keras\r\n",
        "from keras.layers import Input\r\n",
        "from keras import Model\r\n",
        "from keras.utils.vis_utils import plot_model\r\n",
        "from keras.preprocessing import sequence,text\r\n",
        "from keras.preprocessing.text import Tokenizer\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.layers import Dense,Dropout,Embedding,LSTM,Conv1D,GlobalMaxPooling1D,Flatten,MaxPooling1D,GRU,SpatialDropout1D,Bidirectional\r\n",
        "from tensorflow.keras.layers import Bidirectional, GlobalMaxPooling1D, GlobalAveragePooling1D, concatenate,GlobalMaxPool1D\r\n",
        "from keras.callbacks import EarlyStopping\r\n",
        "from keras.utils import to_categorical\r\n",
        "from keras.losses import categorical_crossentropy\r\n",
        "from keras.optimizers import Adam\r\n",
        "from keras.callbacks import Callback\r\n",
        "\r\n",
        "from sklearn.ensemble import VotingClassifier\r\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\r\n",
        "from nltk import pos_tag\r\n",
        "from nltk.tokenize import word_tokenize\r\n",
        " \r\n",
        "from datetime import datetime\r\n",
        "##NLTK library for stemming and lemming\r\n",
        "from nltk.corpus import stopwords \r\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer,LancasterStemmer\r\n",
        "stemmer = LancasterStemmer()\r\n",
        "lemmatizer = WordNetLemmatizer()\r\n",
        "\r\n",
        "import os\r\n",
        "import re\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\r\n",
        "!pip install imblearn\r\n",
        "from imblearn.over_sampling import RandomOverSampler as RandOverSampling\r\n",
        "from imblearn.under_sampling import RandomUnderSampler as RandUnderSampling\r\n",
        "from sklearn.model_selection import KFold\r\n",
        "import lightgbm as lgb\r\n",
        "from sklearn.linear_model import LogisticRegression\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "# CatBoost model\r\n",
        "!pip install catboost\r\n",
        "from catboost import CatBoostClassifier, Pool\r\n",
        "\r\n",
        "from gensim.models import Word2Vec\r\n",
        "from scipy.sparse import hstack\r\n",
        "from sklearn.model_selection import cross_val_score\r\n",
        "from sklearn.metrics import accuracy_score,f1_score,roc_auc_score,precision_score,recall_score\r\n",
        "from sklearn.model_selection import GridSearchCV\r\n",
        "from sklearn.model_selection import StratifiedKFold\r\n",
        "!pip install tensorflow transformers\r\n",
        "import tensorflow as tf\r\n",
        "from tensorflow.keras import activations, optimizers, losses\r\n",
        "from transformers import DistilBertTokenizer, TFDistilBertForSequenceClassification\r\n",
        "\r\n",
        "from keras.preprocessing.text import Tokenizer\r\n",
        "from keras.preprocessing.sequence import pad_sequences\r\n",
        "from nltk.corpus import stopwords\r\n",
        "nltk.download('stopwords')\r\n",
        "nltk.download('wordnet')\r\n",
        "nltk.download('punkt')\r\n",
        "\r\n",
        "! pip install -q scikit-plot\r\n",
        "import scikitplot as skplt\r\n",
        "!pip install tensorflow transformers\r\n",
        "!pip install sentencepiece \r\n",
        "import tensorflow as tf\r\n",
        "from tensorflow.keras import activations, optimizers, losses\r\n",
        "from transformers import DistilBertTokenizer, TFDistilBertForSequenceClassification\r\n",
        "from transformers import AlbertTokenizer, TFAlbertForSequenceClassification\r\n",
        "import sentencepiece as spm"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: imblearn in /usr/local/lib/python3.6/dist-packages (0.0)\n",
            "Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.6/dist-packages (from imblearn) (0.4.3)\n",
            "Requirement already satisfied: numpy>=1.8.2 in /usr/local/lib/python3.6/dist-packages (from imbalanced-learn->imblearn) (1.19.5)\n",
            "Requirement already satisfied: scipy>=0.13.3 in /usr/local/lib/python3.6/dist-packages (from imbalanced-learn->imblearn) (1.4.1)\n",
            "Requirement already satisfied: scikit-learn>=0.20 in /usr/local/lib/python3.6/dist-packages (from imbalanced-learn->imblearn) (0.22.2.post1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.20->imbalanced-learn->imblearn) (1.0.0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/externals/six.py:31: FutureWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
            "  \"(https://pypi.org/project/six/).\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.neighbors.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.neighbors. Anything that cannot be imported from sklearn.neighbors is now part of the private API.\n",
            "  warnings.warn(message, FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Collecting catboost\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/20/37/bc4e0ddc30c07a96482abf1de7ed1ca54e59bba2026a33bca6d2ef286e5b/catboost-0.24.4-cp36-none-manylinux1_x86_64.whl (65.7MB)\n",
            "\u001b[K     |████████████████████████████████| 65.8MB 47kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from catboost) (1.19.5)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.6/dist-packages (from catboost) (4.4.1)\n",
            "Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.6/dist-packages (from catboost) (1.1.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from catboost) (1.4.1)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.6/dist-packages (from catboost) (0.10.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from catboost) (3.2.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from catboost) (1.15.0)\n",
            "Requirement already satisfied: retrying>=1.3.3 in /usr/local/lib/python3.6/dist-packages (from plotly->catboost) (1.3.3)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.24.0->catboost) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.24.0->catboost) (2.8.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->catboost) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->catboost) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->catboost) (0.10.0)\n",
            "Installing collected packages: catboost\n",
            "Successfully installed catboost-0.24.4\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.6/dist-packages (2.4.0)\n",
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/88/b1/41130a228dd656a1a31ba281598a968320283f48d42782845f6ba567f00b/transformers-4.2.2-py3-none-any.whl (1.8MB)\n",
            "\u001b[K     |████████████████████████████████| 1.8MB 11.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy~=1.19.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.19.5)\n",
            "Requirement already satisfied: h5py~=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (2.10.0)\n",
            "Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.12.4)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.12)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.2)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.12.1)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.3.3)\n",
            "Requirement already satisfied: grpcio~=1.32.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.32.0)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: tensorflow-estimator<2.5.0,>=2.4.0rc0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.10.0)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.36.2)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.7.4.3)\n",
            "Requirement already satisfied: tensorboard~=2.4 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.8)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 41.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Collecting tokenizers==0.9.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/1c/e789a8b12e28be5bc1ce2156cf87cb522b379be9cadc7ad8091a4cc107c4/tokenizers-0.9.4-cp36-cp36m-manylinux2010_x86_64.whl (2.9MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9MB 28.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from transformers) (3.3.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.9.2->tensorflow) (51.3.3)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow) (1.0.1)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow) (1.17.2)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow) (0.4.2)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow) (1.7.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow) (3.3.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (4.6)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (4.2.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow) (1.3.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3\"->google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow) (3.1.0)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893261 sha256=2abbafed401366e5823e70f2892d1b413046283fc312f2cb2a9559dc5bac7a2f\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sacremoses, tokenizers, transformers\n",
            "Successfully installed sacremoses-0.0.43 tokenizers-0.9.4 transformers-4.2.2\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.6/dist-packages (2.4.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (4.2.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.5.0,>=2.4.0rc0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.2)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.7.4.3)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: tensorboard~=2.4 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.12.1)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.36.2)\n",
            "Requirement already satisfied: h5py~=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (2.10.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.12.4)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied: grpcio~=1.32.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.32.0)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: numpy~=1.19.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.19.5)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.10.0)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.12)\n",
            "Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.3.3)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from transformers) (3.3.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n",
            "Requirement already satisfied: tokenizers==0.9.4 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.9.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.8)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow) (0.4.2)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow) (1.7.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow) (51.3.3)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow) (3.3.3)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow) (1.17.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.0.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow) (1.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (4.6)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (4.2.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow) (3.1.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3\"->google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (0.4.8)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/67/e42bd1181472c95c8cda79305df848264f2a7f62740995a46945d9797b67/sentencepiece-0.1.95-cp36-cp36m-manylinux2014_x86_64.whl (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 11.4MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.95\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5nc6EmrIUjG6",
        "outputId": "a3c5384b-c44a-4ec1-aec5-d7b99eebcfc2"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-TIbeJ9fW3kB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef87269c-6fac-4e65-d246-f6248b7a18f1"
      },
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-01-26 09:47:41--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2021-01-26 09:47:41--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2021-01-26 09:47:41--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  2.02MB/s    in 6m 27s  \n",
            "\n",
            "2021-01-26 09:54:08 (2.13 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gIgPEpjvW7l6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa8d8da3-7c4b-4748-e908-221907deda33"
      },
      "source": [
        "!unzip glove*.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  glove.6B.zip\n",
            "  inflating: glove.6B.50d.txt        \n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "97uW2L5G-3Oo"
      },
      "source": [
        "def timeLogger(inputString):\r\n",
        "    print(inputString, \" :\", datetime.now())\r\n",
        "\r\n",
        "class dataImports():\r\n",
        "    \r\n",
        "    def __init__(self, df=None):\r\n",
        "        self.df = df\r\n",
        "        \r\n",
        "    def openZipFileCsv(self,zipPath,fileName):\r\n",
        "        zp=zipfile.ZipFile(zipPath,'r')\r\n",
        "        self.df= pd.read_csv(zp.open(fileName))\r\n",
        "        return self\r\n",
        "        \r\n",
        "    def import_Data_File(self,objectName,src_type,sheet=None):   \r\n",
        "        if src_type=='csv':\r\n",
        "            self.df=pd.read_csv(objectName)       \r\n",
        "        if src_type=='excel':\r\n",
        "            self.df=pd.read_excel(objectName, sheet_name = None)\r\n",
        "        return self\r\n",
        "    def import_Data_Database(self,src_type,tbname,server_name,db_name,uname,pwd):\r\n",
        "        if src_type=='mssql':\r\n",
        "            con=getConnectionMSSQL(server_name,db_name,uname,pwd)\r\n",
        "            self.df=selectFromDb(tbname,server_name,db_name,uname,pwd,con)\r\n",
        "        return self\r\n",
        "    \r\n",
        "        \r\n",
        "    def describe(self):              \r\n",
        "        print('------------------------------------DF Head-----------------------------------------')\r\n",
        "        print(self.df.head())\r\n",
        "        print('-------------------------------------Shape------------------------------------------')\r\n",
        "        print(self.df.shape)\r\n",
        "        print('--------------------------------------Info------------------------------------------')\r\n",
        "        print(self.df.info())\r\n",
        "        print('------------------------------DataSet Description-----------------------------------')\r\n",
        "        print(self.df.describe())\r\n",
        "        return self\r\n",
        "\r\n",
        "\r\n",
        "def getThresholdValue(Y_Proba_DataFram,Y_DataFram):\r\n",
        "    fpr, tpr, thresholds = roc_curve(Y_DataFram.astype(np.float32),np.array(Y_Proba_DataFram[:, 1]).astype(np.float32))\r\n",
        "    J = tpr - fpr\r\n",
        "    ix = argmax(J)\r\n",
        "    best_thresh = thresholds[ix]\r\n",
        "    return best_thresh\r\n",
        "\r\n",
        "\r\n",
        "def removeStopWordsList(listInput):\r\n",
        "    #timeLogger(\"Start Time Removing Stop Words\")\r\n",
        "    stp=stopwords.words('english')\r\n",
        "    outputList=[]\r\n",
        "    for token in listInput:\r\n",
        "            fileteredSentence=[w for w in token.split() if not w in stp]\r\n",
        "            outputList.append(' '.join(fileteredSentence))\r\n",
        "    #timeLogger(\"End Time Removing Stop Words\")\r\n",
        "    return outputList\r\n",
        "\r\n",
        "def removeNonNumericLetters(DataFram,column):\r\n",
        "    #timeLogger(\"Start Time Removing Non Numeric and Non Alphabets for \"+column)\r\n",
        "    res=DataFram[column].replace(to_replace =r'[\\W_]+', value = r' ', regex = True).replace(to_replace =r'?', value = r'') \r\n",
        "    #timeLogger(\"End Time Removing Non Numeric and Non Alphabets for \"+column)\r\n",
        "    return res\r\n",
        "\r\n",
        "\r\n",
        "def vectorize_TFIDF(DataFram,column):\r\n",
        "    vectorizer = TfidfVectorizer(analyzer=\"word\")\r\n",
        "    #timeLogger(\"Start Time TF IDF for \"+column)\r\n",
        "    res=vectorizer.fit_transform(DataFram[column]) \r\n",
        "    #timeLogger(\"End Time TF IDF for \"+column)\r\n",
        "    return res\r\n",
        "    \r\n",
        "def lemSentences(datFram,column,dropStopWords=True):\r\n",
        "    #timeLogger(\"Start Time Lemming\")\r\n",
        "    w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\r\n",
        "    lemmatizer = nltk.stem.WordNetLemmatizer()\r\n",
        "    l=list(datFram[column])\r\n",
        "    x=[]\r\n",
        "    lem_List=[]\r\n",
        "    if(dropStopWords):\r\n",
        "        l=removeStopWordsList(l)\r\n",
        "    lem_List=l\r\n",
        "    for token in lem_List:\r\n",
        "        fileteredSentence=[lemmatizer.lemmatize(w) for w in token.split()]\r\n",
        "        x.append(' '.join(fileteredSentence))\r\n",
        "    lem_tokens=x\r\n",
        "    #timeLogger(\"End Time Lemming\")\r\n",
        "    return lem_tokens\r\n",
        "\r\n",
        "def stemSentences(datFram,column,dropStopWords=True):    \r\n",
        "    #timeLogger(\"Start Time Stemming\")\r\n",
        "    porter_stemmer = PorterStemmer()\r\n",
        "    l=list(datFram[column])\r\n",
        "    x=[]\r\n",
        "    lem_List=[]\r\n",
        "    if(dropStopWords):\r\n",
        "        l=removeStopWordsList(l)\r\n",
        "    lem_List=l\r\n",
        "    for token in lem_List:\r\n",
        "        fileteredSentence=[porter_stemmer.stem(w) for w in token.split()]\r\n",
        "        x.append(' '.join(fileteredSentence))\r\n",
        "    stemmed_tokens=x\r\n",
        "    #timeLogger(\"End Time Stemming\")\r\n",
        "    return stemmed_tokens\r\n",
        "\r\n",
        "\r\n",
        "def lemmatize_all(sentence):\r\n",
        "    #timeLogger(\"Start Time Lemming\")\r\n",
        "    wnl = WordNetLemmatizer()\r\n",
        "    for word, tag in pos_tag(word_tokenize(sentence)):\r\n",
        "        if tag.startswith(\"NN\"):\r\n",
        "            yield wnl.lemmatize(word, pos='n')\r\n",
        "        elif tag.startswith('VB'):\r\n",
        "            yield wnl.lemmatize(word, pos='v')\r\n",
        "        elif tag.startswith('JJ'):\r\n",
        "            yield wnl.lemmatize(word, pos='a')\r\n",
        "        else:\r\n",
        "            yield word\r\n",
        "    #timeLogger(\"End Time Lemming\")\r\n",
        "\r\n",
        "def stem_all(sentence):\r\n",
        "    #timeLogger(\"Start Time Stemming\")\r\n",
        "    porter_stemmer = PorterStemmer()\r\n",
        "    for word, tag in pos_tag(word_tokenize(sentence)):\r\n",
        "        if tag.startswith(\"NN\"):\r\n",
        "            yield porter_stemmer.stem(word, pos='n')\r\n",
        "        elif tag.startswith('VB'):\r\n",
        "            yield porter_stemmer.stem(word, pos='v')\r\n",
        "        elif tag.startswith('JJ'):\r\n",
        "            yield porter_stemmer.stem(word, pos='a')\r\n",
        "        else:\r\n",
        "            yield word\r\n",
        "    #timeLogger(\"End Time Stemming\")\r\n",
        "\r\n",
        "\r\n",
        "def posSentences(datFram,column,typ,dropStopWords=True):   \r\n",
        "    #timeLogger(\"Start Time POS\")\r\n",
        "    l=list(datFram[column])\r\n",
        "    x=[]\r\n",
        "    lem_List=[]\r\n",
        "    if(dropStopWords):\r\n",
        "        l=removeStopWordsList(l)\r\n",
        "    lem_List=l\r\n",
        "    for token in lem_List:\r\n",
        "        if(typ=='lem'):\r\n",
        "            tokens_pos=' '.join(lemmatize_all(token))\r\n",
        "            x.append(tokens_pos)\r\n",
        "        \r\n",
        "        elif(typ=='stem'):\r\n",
        "            tokens_pos=' '.join(stem_all(token))\r\n",
        "            x.append(tokens_pos)\r\n",
        "    stemmed_tokens=x\r\n",
        "    #timeLogger(\"End Time POS\")\r\n",
        "    return stemmed_tokens\r\n",
        "\r\n",
        "def dataLowerCase(DataFram,dataFrameName):\r\n",
        "    print(\"Invoking lower casing\")\r\n",
        "    #timeLogger(\"Start Time LowerCasing \" +dataFrameName)\r\n",
        "    result = DataFram.apply(lambda x: x.astype(str).str.lower()).apply(lambda x: x.astype(str).str.encode('ascii', 'ignore').str.decode('ascii'))\r\n",
        "    #timeLogger(\"End Time LowerCasing \"+dataFrameName)\r\n",
        "    return result\r\n",
        "\r\n",
        "def vectorizeCount(DataFram,column1,column2,dropNonNumericLetters,preProcess,ngram):\r\n",
        "    #timeLogger(\"Start Time Count Vectorizer\")\r\n",
        "    vectorizer = CountVectorizer(analyzer=\"word\",max_features=200)\r\n",
        "    if(ngram==1):\r\n",
        "        vectorizer = CountVectorizer(analyzer=\"word\",ngram_range=(1,1),max_features=200)    \r\n",
        "    if(ngram==2):\r\n",
        "        vectorizer = CountVectorizer(analyzer=\"word\",ngram_range=(2,2),max_features=200)   \r\n",
        "    if(ngram==3):\r\n",
        "        vectorizer = CountVectorizer(analyzer=\"word\",ngram_range=(3,3),max_features=200)    \r\n",
        "    if(dropNonNumericLetters):\r\n",
        "        DataFram[column1]=removeNonNumericLetters(DataFram,column1)\r\n",
        "        DataFram[column2]=removeNonNumericLetters(DataFram,column2)\r\n",
        "    if(preProcess=='stem'): \r\n",
        "        DataFram[column1]=stemSentences(DataFram,column1,dropNonNumericLetters)\r\n",
        "        DataFram[column2]=stemSentences(DataFram,column2,dropNonNumericLetters)\r\n",
        "    if(preProcess=='lem'): \r\n",
        "        DataFram[column1]=lemSentences(DataFram,column1,dropNonNumericLetters)\r\n",
        "        DataFram[column2]=lemSentences(DataFram,column2,dropNonNumericLetters)\r\n",
        "    if(preProcess=='poslem'): \r\n",
        "        DataFram[column1]=posSentences(DataFram,column1,'lem',dropNonNumericLetters)\r\n",
        "        DataFram[column2]=posSentences(DataFram,column2,'lem',dropNonNumericLetters)\r\n",
        "    if(preProcess=='posstem'): \r\n",
        "        DataFram[column1]=posSentences(DataFram,column1,'stem',dropNonNumericLetters)\r\n",
        "        DataFram[column2]=posSentences(DataFram,column2,'stem',dropNonNumericLetters)\r\n",
        "    voc=vectorizer.fit(pd.concat([DataFram[column1],DataFram[column2]]))\r\n",
        "    res1=voc.transform(DataFram[column1])\r\n",
        "    res2=voc.transform(DataFram[column2])\r\n",
        "#    res=[i + j for i, j in zip(res1, res2)] \r\n",
        "#    return res\r\n",
        "    #timeLogger(\"END Time Count Vectorizer\")\r\n",
        "    return hstack((res1,res2)).toarray()\r\n",
        "\r\n",
        "\r\n",
        "    \r\n",
        "\r\n",
        "def vectorizeTFIDF(DataFram,column1,column2,dropNonNumericLetters,preProcess,ngram):\r\n",
        "    \r\n",
        "    #timeLogger(\"Start Time TFIDF\")\r\n",
        "    vectorizer = TfidfVectorizer(analyzer=\"word\",max_features=200)\r\n",
        "    if(ngram==1):\r\n",
        "        vectorizer = TfidfVectorizer(analyzer=\"word\",ngram_range=(1,1),max_features=200)    \r\n",
        "    if(ngram==2):\r\n",
        "        vectorizer = TfidfVectorizer(analyzer=\"word\",ngram_range=(2,2),max_features=200)   \r\n",
        "    if(ngram==3):\r\n",
        "        vectorizer = TfidfVectorizer(analyzer=\"word\",ngram_range=(3,3),max_features=200)\r\n",
        "       \r\n",
        "    if(dropNonNumericLetters):\r\n",
        "        DataFram[column1]=removeNonNumericLetters(DataFram,column1)\r\n",
        "        DataFram[column2]=removeNonNumericLetters(DataFram,column2)\r\n",
        "    if(preProcess=='stem'): \r\n",
        "        DataFram[column1]=stemSentences(DataFram,column1,dropNonNumericLetters)\r\n",
        "        DataFram[column2]=stemSentences(DataFram,column2,dropNonNumericLetters)\r\n",
        "    if(preProcess=='lem'): \r\n",
        "        DataFram[column1]=lemSentences(DataFram,column1,dropNonNumericLetters)\r\n",
        "        DataFram[column2]=lemSentences(DataFram,column2,dropNonNumericLetters)\r\n",
        "    if(preProcess=='pos'): \r\n",
        "        DataFram[column1]=posSentences(DataFram,column1,dropNonNumericLetters)\r\n",
        "        DataFram[column2]=posSentences(DataFram,column2,dropNonNumericLetters)\r\n",
        "    voc=vectorizer.fit(pd.concat([DataFram[column1],DataFram[column2]]))\r\n",
        "    res1=voc.transform(DataFram[column1])\r\n",
        "    res2=voc.transform(DataFram[column2])\r\n",
        "#    res=[i + j for i, j in zip(res1, res2)] \r\n",
        "#    return res\r\n",
        "    #timeLogger(\"Start Time TFIDF\")\r\n",
        "    return hstack((res1,res2)).toarray()\r\n",
        "\r\n",
        "def loadEmbeddings(path,encode):\r\n",
        "    # load the whole embedding into memory\r\n",
        "    #timeLogger(\"Start Time Load embedding\")\r\n",
        "    embeddings_index = dict()\r\n",
        "    f = open(path,encoding=encode)\r\n",
        "\r\n",
        "    for line in f:\r\n",
        "        values = line.split()\r\n",
        "        word = values[0]\r\n",
        "        coefs = np.asarray(values[1:], dtype='float32')\r\n",
        "        embeddings_index[word] = coefs\r\n",
        "\r\n",
        "    f.close()\r\n",
        "    print('Loaded %s word vectors.' % len(embeddings_index))\r\n",
        "    #timeLogger(\"End Time Load embedding\")\r\n",
        "    return embeddings_index\r\n",
        "\r\n",
        "\r\n",
        "def usePretrainedEmbedding(Embedding,DataFram,column1,column2,embeddingSize,dropNonNumericLetters,preProcess):\r\n",
        "    #timeLogger(\"Start Time Pretrained embedding\")\r\n",
        "    tokenizer = Tokenizer()\r\n",
        "    tokenizer.fit_on_texts(list(pd.concat([DataFram[column1],DataFram[column2]])))\r\n",
        "    if(dropNonNumericLetters):\r\n",
        "        DataFram[column1]=removeNonNumericLetters(DataFram,column1)\r\n",
        "        DataFram[column2]=removeNonNumericLetters(DataFram,column2)\r\n",
        "    if(preProcess=='stem'): \r\n",
        "        DataFram[column1]=stemSentences(DataFram,column1,dropNonNumericLetters)\r\n",
        "        DataFram[column2]=stemSentences(DataFram,column2,dropNonNumericLetters)\r\n",
        "    if(preProcess=='lem'): \r\n",
        "        DataFram[column1]=lemSentences(DataFram,column1,dropNonNumericLetters)\r\n",
        "        DataFram[column2]=lemSentences(DataFram,column2,dropNonNumericLetters)\r\n",
        "    if(preProcess=='poslem'): \r\n",
        "        DataFram[column1]=posSentences(DataFram,column1,'lem',dropNonNumericLetters)\r\n",
        "        DataFram[column2]=posSentences(DataFram,column2,'lem',dropNonNumericLetters)\r\n",
        "    if(preProcess=='posstem'): \r\n",
        "        DataFram[column1]=posSentences(DataFram,column1,'stem',dropNonNumericLetters)\r\n",
        "        DataFram[column2]=posSentences(DataFram,column2,'stem',dropNonNumericLetters)\r\n",
        "    \r\n",
        "    embedding=Embedding\r\n",
        "    #converting text into integer sequences \r\n",
        "    df_train_all= list(zip(list(DataFram[column1]),list(DataFram[column2])))\r\n",
        "    R=[]\r\n",
        "    for i in df_train_all:\r\n",
        "        M=[]\r\n",
        "        words=word_tokenize(str(i))\r\n",
        "        for w in words:\r\n",
        "            try:\r\n",
        "                M.append(embedding[str(w)])\r\n",
        "            except:\r\n",
        "                continue\r\n",
        "            X=np.array(M)\r\n",
        "            v = X.sum(axis=0)\r\n",
        "            if type(v) != np.ndarray:\r\n",
        "                u= np.zeros(300)\r\n",
        "            u=v / np.sqrt((v ** 2).sum())\r\n",
        "        R.append(u)\r\n",
        "   # df_train_all  = tokenizer.texts_to_sequences(df_train[column1])   \r\n",
        "    #padding to prepare sequences of same length\r\n",
        "    \r\n",
        "    df_train_all  = np.array(R)\r\n",
        "    #timeLogger(\"END Time Pretrained embedding\")\r\n",
        "    return(df_train_all)\r\n",
        "\r\n",
        "def sampling(X,y,split):\r\n",
        "    #timeLogger(\"Start Time Sampling\")\r\n",
        "    if(split=='N'):\r\n",
        "        kfolds = KFold(10)\r\n",
        "    elif(split=='stratified'):\r\n",
        "        kfolds = StratifiedKFold(10)\r\n",
        "    elif(split=='underSample'):\r\n",
        "        undersample = RandUnderSampling(sampling_strategy='majority')\r\n",
        "        X,y=undersample.fit_resample(X, y)\r\n",
        "        kfolds = KFold(n_splits=10)\r\n",
        "    elif(split=='overSample'):\r\n",
        "        undersample = RandOverSampling(sampling_strategy='minority')\r\n",
        "        X,y=undersample.fit_resample(X, y)\r\n",
        "        kfolds = KFold(n_splits=10)\r\n",
        "    #timeLogger(\"END Time Sampling\")\r\n",
        "    return kfolds\r\n",
        "\r\n",
        "def evalMetrics(y_train,y_pred_train,y_test,y_pred_test):\r\n",
        "  lst=[]  \r\n",
        "  lst.append(accuracy_score(y_train,y_pred_train))\r\n",
        "  lst.append(accuracy_score(y_test,y_pred_test))\r\n",
        "  lst.append(f1_score(y_train,y_pred_train, average='weighted'))\r\n",
        "  lst.append(f1_score(y_test,y_pred_test, average='weighted'))\r\n",
        "  lst.append(roc_auc_score(y_train,y_pred_train))\r\n",
        "  lst.append(roc_auc_score(y_test,y_pred_test))\r\n",
        "  lst.append(precision_score(y_train,y_pred_train, average='weighted'))\r\n",
        "  lst.append(precision_score(y_test,y_pred_test, average='weighted'))\r\n",
        "  lst.append(recall_score(y_train,y_pred_train, average='weighted'))\r\n",
        "  lst.append(recall_score(y_test,y_pred_test, average='weighted'))   \r\n",
        "  return lst\r\n",
        "\r\n",
        "def getEmbeddingSet(vect,DataFram,column1,column2,dropNonNumericLetters,preProcess,ngram,embeddingSize):\r\n",
        "  if(vect=='cnt'):\r\n",
        "        X=vectorizeCount(DataFram,column1,column2,True,preProcess,ngram)\r\n",
        "  elif(vect=='tfidf'):\r\n",
        "      X=vectorizeTFIDF(DataFram,column1,column2,True,preProcess,ngram)\r\n",
        "  elif(vect=='glove100'):\r\n",
        "      embedding=loadEmbeddings('./glove.6B.100d.txt','UTF-8')\r\n",
        "      X=usePretrainedEmbedding(embedding,DataFram,column1,column2,embeddingSize,True,preProcess)\r\n",
        "  elif(vect=='glove200'):\r\n",
        "      embedding=loadEmbeddings('./glove.6B.200d.txt','UTF-8')\r\n",
        "      X=usePretrainedEmbedding(embedding,DataFram,column1,column2,embeddingSize,True,preProcess)\r\n",
        "  elif(vect=='glove300'):\r\n",
        "      embedding=loadEmbeddings('./glove.6B.300d.txt','UTF-8')\r\n",
        "      X=usePretrainedEmbedding(embedding,DataFram,column1,column2,embeddingSize,True,preProcess)\r\n",
        "  elif(vect=='word2vec'):\r\n",
        "      all_words=[]\r\n",
        "      for i in pd.concat([DataFram[column1],DataFram[column2]]):\r\n",
        "        for x in i.split():\r\n",
        "          all_words.append(str(x))\r\n",
        "      all_words=list(set(all_words))\r\n",
        "      embedding=Word2Vec(all_words, min_count=2)\r\n",
        "      X=usePretrainedEmbedding(embedding,DataFram,column1,column2,embeddingSize,True,preProcess)\r\n",
        "  return X    \r\n",
        "\r\n",
        "def pltConfusion(y_actual,y_pred,type1,inputMethod):\r\n",
        "  skplt.metrics.plot_confusion_matrix(y_actual, y_pred,figsize=(12,12), normalize='all')\r\n",
        "  plt.savefig('/content/drive/MyDrive/LJMU Masters/code/img'+\"\".join(inputMethod)+'_'+type1+'.png')\r\n",
        "\r\n",
        "def MlModel(DataFram,column1,column2,label,dropNonNumericLetters,preProcess,vect,ngram=0,embeddingSize=100,split='N',model_type='lr'):    \r\n",
        "    timeLogger(\"Start Time ML Model\")\r\n",
        "    start_train_time = time()\r\n",
        "    X=getEmbeddingSet(vect,DataFram,column1,column2,dropNonNumericLetters==True,preProcess,ngram,embeddingSize)\r\n",
        "    y = np.array(DataFram[label])\r\n",
        "    embeddingTime=time() - start_train_time\r\n",
        "    if(model_type=='lr'):\r\n",
        "        mlr = LogisticRegression(max_iter=4000,n_jobs=-1,solver='sag', tol=1e-1, C=1.e4 / X.shape[0])\r\n",
        "    if(model_type=='gru'):\r\n",
        "        mlr = LogisticRegression(max_iter=4000,n_jobs=-1,solver='sag', tol=1e-1, C=1.e4 / X.shape[0])\r\n",
        "    elif(model_type=='cat'):\r\n",
        "        mlr = CatBoostClassifier(iterations=50 #learning_rate = 0.01,od_type = \"Iter\",early_stopping_rounds = 10\r\n",
        "          )\r\n",
        "    elif(model_type=='light'):\r\n",
        "        mlr = lgb.LGBMClassifier(max_depth=-1, random_state=314, silent=True, metric='None', \r\n",
        "                                 n_jobs=4, n_estimators=500)\r\n",
        "    return mlr,X,y,split,embeddingTime,list([model_type,preProcess,vect,split])\r\n",
        "\r\n",
        "def runMlModel(mlr,X,y,split,embeddingTime,inputMethod):    \r\n",
        "    parameters={}\r\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, stratify=y)\r\n",
        "    kfolds=sampling(X_train,y_train,split)\r\n",
        "    clf = GridSearchCV(mlr, parameters, cv=kfolds.split(X_train,y_train))\r\n",
        "    start_train_time = time()\r\n",
        "    mdl=clf.fit(X_train,y_train)\r\n",
        "    trainingTime=time() - start_train_time\r\n",
        "    timeLogger(\"END Time ML modelling\")\r\n",
        "    start_predtrain_time = time()\r\n",
        "    y_pred_train = mdl.predict(X_train)\r\n",
        "    predTrainingTime=time() - start_predtrain_time\r\n",
        "    start_predtrain_time = time()\r\n",
        "    y_pred_test = mdl.predict(X_test)\r\n",
        "    predTestTime=time() - start_predtrain_time\r\n",
        "    #cross_val_score(mlr, X, y, cv=10, scoring='recall_macro')\r\n",
        "    lst=evalMetrics(y_train,y_pred_train,y_test,y_pred_test)\r\n",
        "    lst.append(embeddingTime)\r\n",
        "    lst.append(trainingTime)\r\n",
        "    lst.append(predTrainingTime)\r\n",
        "    lst.append(predTestTime)\r\n",
        "\r\n",
        "    pltConfusion(y_train, y_pred_train,'train',inputMethod)\r\n",
        "    pltConfusion(y_test, y_pred_test,'test',inputMethod)\r\n",
        "    \r\n",
        "    return lst\r\n",
        "\r\n",
        "def runMlEnsembleModel(mlr,X,y,split,embeddingTime):    \r\n",
        "    parameters={}\r\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, stratify=y)\r\n",
        "    kfolds=sampling(X_train,y_train,split)\r\n",
        "    clf = GridSearchCV(mlr, parameters, cv=kfolds.split(X_train,y_train))\r\n",
        "    start_train_time = time()\r\n",
        "    mdl=clf.fit(X_train,y_train)\r\n",
        "    trainingTime=time() - start_train_time\r\n",
        "    timeLogger(\"END Time ML modelling\")\r\n",
        "    start_predtrain_time = time()\r\n",
        "    y_pred_train = mdl.predict(X_train)\r\n",
        "    predTrainingTime=time() - start_predtrain_time\r\n",
        "    start_predtrain_time = time()\r\n",
        "    y_pred_test = mdl.predict(X_test)\r\n",
        "    predTestTime=time() - start_predtrain_time\r\n",
        "    #cross_val_score(mlr, X, y, cv=10, scoring='recall_macro')\r\n",
        "    lst=evalMetrics(y_train,y_pred_train,y_test,y_pred_test)\r\n",
        "    lst.append(embeddingTime)\r\n",
        "    lst.append(trainingTime)\r\n",
        "    lst.append(predTrainingTime)\r\n",
        "    lst.append(predTestTime)\r\n",
        "    \r\n",
        "    pltConfusion(y_train, y_pred_train,'train',inputMethod)\r\n",
        "    pltConfusion(y_test, y_pred_test,'test',inputMethod)\r\n",
        "    \r\n",
        "    return y_pred_test,y_test,y_pred_train,y_train,X_test\r\n",
        "def GruModel(mlr,X,y,split,embeddingTime):\r\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.50, stratify=y)\r\n",
        "  opt = keras.optimizers.Adam(learning_rate=0.0001)\r\n",
        "  model = Sequential()\r\n",
        "  model.add(X_train)\r\n",
        "  model.add(Bidirectional(GRU(300, return_sequences = True)))\r\n",
        "  model.add(GlobalMaxPooling1D())\r\n",
        "  model.add(Dense(16,activation='relu'))\r\n",
        "  model.add(Dropout(0.1))\r\n",
        "  model.add(Dense(1,activation='sigmoid'))\r\n",
        "  model.compile(loss = 'binary_crossentropy', optimizer=opt,metrics = ['accuracy'])\r\n",
        "  print(model.summary())\r\n",
        "  mdl=model.fit(X_train, y_train,epochs=5, batch_size=128, verbose=1)\r\n",
        "  return mdl\r\n",
        "\r\n",
        "def EnsembleModel(ensembleType,DataFram,column1,column2,label,dropNonNumericLetters,preProcess,vect,ngram=0,embeddingSize=100,split=None,model1=None,model2=None,model3=None):    \r\n",
        "    timeLogger(\"Start Time ML Model\")\r\n",
        "    start_train_time = time()\r\n",
        "    X=getEmbeddingSet(vect,DataFram,column1,column2,dropNonNumericLetters==True,preProcess,ngram,embeddingSize)\r\n",
        "    y = np.array(DataFram[label])\r\n",
        "    embeddingTime=time() - start_train_time\r\n",
        "    mlr1,X,y,split,embeddingTime,inputs = MlModel(DataFram,column1,column2,label,dropNonNumericLetters,preProcess,vect,ngram=0,embeddingSize=100,split='N',model_type='lr')\r\n",
        "    mlr2,X,y,split,embeddingTime,inputs = MlModel(DataFram,column1,column2,label,dropNonNumericLetters,preProcess,vect,ngram=0,embeddingSize=100,split='N',model_type='lr')\r\n",
        "    mlr3,X,y,split,embeddingTime,inputs = MlModel(DataFram,column1,column2,label,dropNonNumericLetters,preProcess,vect,ngram=0,embeddingSize=100,split='N',model_type='lr')\r\n",
        "\r\n",
        "    parameters={}\r\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.75, stratify=y)\r\n",
        "    kfolds=sampling(X_train,y_train,split)\r\n",
        "    start_train_time = time()\r\n",
        "    clf = VotingClassifier(estimators=[('l1', mlr1), ('l2', mlr2), ('l3', mlr3)], voting=ensembleType)\r\n",
        "    trainingTime=time() - start_train_time\r\n",
        "    mdl=clf.fit(X_train,y_train)\r\n",
        "    timeLogger(\"END Time ML modelling\")\r\n",
        "    start_predtrain_time = time()\r\n",
        "    y_pred_train = mdl.predict(X_train)\r\n",
        "    predTrainingTime=time() - start_predtrain_time\r\n",
        "    start_predtrain_time = time()\r\n",
        "    y_pred_test = mdl.predict(X_test)\r\n",
        "    predTestTime=time() - start_predtrain_time\r\n",
        "    #cross_val_score(mlr, X, y, cv=10, scoring='recall_macro')\r\n",
        "    lst=evalMetrics(y_train,y_pred_train,y_test,y_pred_test)\r\n",
        "    lst.append(embeddingTime)\r\n",
        "    lst.append(trainingTime)\r\n",
        "    lst.append(predTrainingTime)\r\n",
        "    lst.append(predTestTime)\r\n",
        "    \r\n",
        "    pltConfusion(y_train, y_pred_train,'train',inputMethod)\r\n",
        "    pltConfusion(y_test, y_pred_test,'test',inputMethod)\r\n",
        "    \r\n",
        "    return lst\r\n",
        "\r\n",
        "def importData(train_loc,train_file,test_loc,test_file,LowerCase):\r\n",
        "    df_train=(dataImports().openZipFileCsv(train_loc,train_file)).df\r\n",
        "    \r\n",
        "    df_test=(dataImports().openZipFileCsv(test_loc,test_file)).df\r\n",
        "    print()\r\n",
        "    if(LowerCase):\r\n",
        "        df_train=dataLowerCase(df_train,\"train Set\")\r\n",
        "        df_test=dataLowerCase(df_test,\"test Set\")\r\n",
        "    return df_train,df_test\r\n",
        "\r\n",
        "def runModel(model_type,preProcess,vect,embeddingSize=100,split='N'):\r\n",
        "  # df_train,df_test=importData('/content/drive/MyDrive/LJMU Masters/code/train.csv.zip','train.csv','/content/drive/MyDrive/LJMU Masters/code/test.csv.zip','test.csv',True)\r\n",
        "  mlr,X,y,split,embeddingTime,inputs=MlModel(df_train,'question1','question2','is_duplicate',True,preProcess,vect,0,embeddingSize,split,model_type)\r\n",
        "  mdl=runMlModel(mlr,X,y,split,embeddingTime,inputs)\r\n",
        "  return mdl\r\n",
        "def runModelEnsemble(ensembleType,preProcess,vect,embeddingSize=100,split='N',model1=None,model2=None,model3=None):\r\n",
        "  df_train,df_test=importData('/content/drive/MyDrive/LJMU Masters/code/train.csv.zip','train.csv','/content/drive/MyDrive/LJMU Masters/code/test.csv.zip','test.csv',True)\r\n",
        "  if(ensembleType=='hard'):\r\n",
        "    mdl=EnsembleModel(ensembleType,df_train,'question1','question2','is_duplicate',True,preProcess,vect,ngram=0,embeddingSize=100,split='N',model1='lr',model2='lr',model3='lr')  \r\n",
        "  if(ensembleType=='stacking1'):\r\n",
        "    mlr_1,X_1,y_1,split,embeddingTime,inputs=MlModel(df_train,'question1','question2','is_duplicate',True,preProcess,vect,0,embeddingSize,split,model1)\r\n",
        "    mdl_1=runMlModel(mlr_1,X_1,y_1,split,embeddingTime,inputs)\r\n",
        "    pred_test_1,test_pred,pred_train_1,train_pred,X_test=runMlEnsembleModel(mlr_1,X_1,y_1,split,embeddingTime)\r\n",
        "    test_pred_1=np.column_stack((test_pred,pred_test_1))\r\n",
        "    train_pred_1=np.column_stack((train_pred,pred_train_1))\r\n",
        "    mlr_2,X_2,y_2,split,embeddingTime,inputs=MlModel(df_train,'question1','question2','is_duplicate',True,preProcess,vect,0,embeddingSize,split,model1)\r\n",
        "    mdl_2=runMlModel(mlr_2,X_2,y_2,split,embeddingTime,inputs)\r\n",
        "    pred_test_2,test_pred,pred_train_2,train_pred,X_test=runMlEnsembleModel(mlr_2,X_2,y_2,split,embeddingTime)\r\n",
        "    test_pred_2=np.column_stack((test_pred,pred_test_2))\r\n",
        "    train_pred_2=np.column_stack((train_pred,pred_train_2))\r\n",
        "    df = pd.concat([pd.DataFrame(train_pred_1), pd.DataFrame(train_pred_2)], axis=1)\r\n",
        "    df_test = pd.concat([pd.DataFrame(test_pred_1), pd.DataFrame(test_pred_2)], axis=1)\r\n",
        "    model = LogisticRegression(random_state=1)\r\n",
        "    model.fit(df,train_pred)\r\n",
        "    print(str(model.score(df_test, test_pred)))\r\n",
        "    t_pred_train=model.predict(df)\r\n",
        "    t_pred_test=model.predict(df_test)\r\n",
        "    lst=evalMetrics(train_pred,t_pred_train,test_pred,t_pred_test)\r\n",
        "  if(ensembleType=='stacking'):\r\n",
        "    mlr1,X,y,split,embeddingTime=MlModel(df_train,'question1','question2','is_duplicate',True,preProcess,vect,0,embeddingSize,split,model1)\r\n",
        "    mlr2,X,y,split,embeddingTime=MlModel(df_train,'question1','question2','is_duplicate',True,preProcess,vect,0,embeddingSize,split,model1)\r\n",
        "    mlr3,X,y,split,embeddingTime=MlModel(df_train,'question1','question2','is_duplicate',True,preProcess,vect,0,embeddingSize,split,model1)\r\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.50, stratify=y)\r\n",
        "    level0 = list()\r\n",
        "    level0.append((model1, mlr1))\r\n",
        "    level0.append((model2, mlr2))\r\n",
        "    level0.append((model3, mlr3))\r\n",
        "    \r\n",
        "    # define meta learner model\r\n",
        "    level1 = LogisticRegression()\r\n",
        "    # define the stacking ensemble\r\n",
        "    mdl = StackingClassifier(estimators=level0, final_estimator=level1, cv=5)\r\n",
        "    # fit the model on all available data\r\n",
        "    mdl.fit(X_train, y_train)\r\n",
        "    \r\n",
        "    timeLogger(\"END Time ML modelling\")\r\n",
        "    start_predtrain_time = time()\r\n",
        "    y_pred_train = mdl.predict(X_train)\r\n",
        "    predTrainingTime=time() - start_predtrain_time\r\n",
        "    start_predtrain_time = time()\r\n",
        "    y_pred_test = mdl.predict(X_test)\r\n",
        "    predTestTime=time() - start_predtrain_time\r\n",
        "    #cross_val_score(mlr, X, y, cv=10, scoring='recall_macro')\r\n",
        "    lst=evalMetrics(y_train,y_pred_train,y_test,y_pred_test)\r\n",
        "    lst.append(embeddingTime)\r\n",
        "    lst.append(trainingTime)\r\n",
        "    lst.append(predTrainingTime)\r\n",
        "    lst.append(predTestTime)\r\n",
        "    \r\n",
        "    pltConfusion(y_train, y_pred_train,'train',inputMethod)\r\n",
        "    pltConfusion(y_test, y_pred_test,'test',inputMethod)\r\n",
        "\r\n",
        "    return lst\r\n",
        "def runGRU(model_type,preProcess,vect,embeddingSize=100,split='N'):\r\n",
        "  df_train,df_test=importData('/content/drive/MyDrive/LJMU Masters/code/train.csv.zip','train.csv','/content/drive/MyDrive/LJMU Masters/code/test.csv.zip','test.csv',True)\r\n",
        "  mlr,X,y,split,embeddingTime=MlModel(df_train,'question1','question2','is_duplicate',True,preProcess,vect,0,embeddingSize,split,model_type)\r\n",
        "  mdl=GruModel(mlr,X,y,split,embeddingTime)  \r\n",
        "from keras import backend as K\r\n",
        "\r\n",
        "def recall_m(y_true, y_pred):\r\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\r\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\r\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\r\n",
        "    return recall\r\n",
        "\r\n",
        "def precision_m(y_true, y_pred):\r\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\r\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\r\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\r\n",
        "    return precision\r\n",
        "\r\n",
        "def f1_m(y_true, y_pred):\r\n",
        "    precision = precision_m(y_true, y_pred)\r\n",
        "    recall = recall_m(y_true, y_pred)\r\n",
        "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5fYPJCghRAOm"
      },
      "source": [
        "#mdl=runModel('light','lem','tfidf',200,'stratified')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yb0vf00sziRJ"
      },
      "source": [
        "#mdl=runModel('light','lem','glove300',300,'stratified')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UfhZSrnkxIPA",
        "outputId": "d37ec921-88af-4af5-8630-219cef907e39"
      },
      "source": [
        "!pip install tensorflow transformers\r\n",
        "!pip install sentencepiece \r\n",
        "import tensorflow as tf\r\n",
        "from tensorflow.keras import activations, optimizers, losses\r\n",
        "from transformers import DistilBertTokenizer, TFDistilBertForSequenceClassification,MobileBertTokenizer, TFMobileBertForSequenceClassification\r\n",
        "from transformers import AlbertTokenizer, TFAlbertForSequenceClassification,SqueezeBertTokenizer, SqueezeBertForSequenceClassification\r\n",
        "import sentencepiece as spm"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.6/dist-packages (2.4.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (4.2.2)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.3.3)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.2)\n",
            "Requirement already satisfied: numpy~=1.19.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.19.5)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.5.0,>=2.4.0rc0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.12.1)\n",
            "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.10.0)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.7.4.3)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.12.4)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.12)\n",
            "Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied: tensorboard~=2.4 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.36.2)\n",
            "Requirement already satisfied: grpcio~=1.32.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.32.0)\n",
            "Requirement already satisfied: h5py~=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (2.10.0)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from transformers) (3.3.0)\n",
            "Requirement already satisfied: tokenizers==0.9.4 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.9.4)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.8)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.9.2->tensorflow) (51.3.3)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow) (1.17.2)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow) (1.7.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow) (0.4.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow) (3.3.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.0.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (4.6)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (4.2.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow) (1.3.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3\"->google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow) (3.1.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (0.1.95)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ac8fIgWRTwCe",
        "outputId": "68a6a7bb-0c9c-49f2-cd91-f874b947e2ec"
      },
      "source": [
        "df_train,df_test=importData('/content/drive/MyDrive/LJMU Masters/code/train.csv.zip','train.csv','/content/drive/MyDrive/LJMU Masters/code/test.csv.zip','test.csv',True)\r\n",
        "X=list('[CLS] '+df_train.question1+' [SEP] '+df_train.question2+' [SEP] ')\r\n",
        "y=df_train['is_duplicate']\r\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.90, stratify=y)\r\n",
        "from keras import backend as K\r\n",
        "\r\n",
        "def recall_m(y_true, y_pred):\r\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\r\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\r\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\r\n",
        "    return recall\r\n",
        "\r\n",
        "def precision_m(y_true, y_pred):\r\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\r\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\r\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\r\n",
        "    return precision\r\n",
        "\r\n",
        "def f1_m(y_true, y_pred):\r\n",
        "    precision = precision_m(y_true, y_pred)\r\n",
        "    recall = recall_m(y_true, y_pred)\r\n",
        "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\r\n",
        "def runTransfomerModel(modelName,maxLen,X_train,y_train):\r\n",
        "  if(modelName=='distil'):\r\n",
        "    MODEL_NAME = 'distilbert-base-uncased'\r\n",
        "    tkzr = DistilBertTokenizer.from_pretrained(MODEL_NAME)\r\n",
        "    model = TFDistilBertForSequenceClassification.from_pretrained(MODEL_NAME)\r\n",
        "  if(modelName=='albert'):\r\n",
        "    MODEL_NAME = 'albert-base-v2'\r\n",
        "    tkzr = AlbertTokenizer.from_pretrained(MODEL_NAME)\r\n",
        "    model = TFAlbertForSequenceClassification.from_pretrained(MODEL_NAME)\r\n",
        "  if(modelName=='mobile'):\r\n",
        "    MODEL_NAME = 'google/mobilebert-uncased'\r\n",
        "    tkzr = MobileBertTokenizer.from_pretrained(MODEL_NAME)\r\n",
        "    model = TFMobileBertForSequenceClassification.from_pretrained(MODEL_NAME)\r\n",
        "  if(modelName=='squeeze'):\r\n",
        "    MODEL_NAME = 'squeezebert/squeezebert-mnli-headless'\r\n",
        "    tkzr = SqueezeBertTokenizer.from_pretrained(MODEL_NAME)\r\n",
        "    model = SqueezeBertForSequenceClassification.from_pretrained(MODEL_NAME)\r\n",
        "  MAX_LEN = maxLen\r\n",
        "  y_train = list(map(int, y_train)) \r\n",
        "  encodings = tkzr(X_train, max_length=maxLen, truncation=True, padding=True)\r\n",
        "  tfdataset = tf.data.Dataset.from_tensor_slices((dict(encodings),y_train))\r\n",
        "  TEST_SPLIT = 0.50\r\n",
        "  BATCH_SIZE = 16\r\n",
        "  train_size = int(len(X_train) * (1-TEST_SPLIT))\r\n",
        "  tfdataset = tfdataset.shuffle(len(X_train))\r\n",
        "  tfdataset_train = tfdataset.take(train_size)\r\n",
        "  tfdataset_test = tfdataset.skip(train_size)\r\n",
        "  tfdataset_train = tfdataset_train.batch(BATCH_SIZE)\r\n",
        "  tfdataset_test = tfdataset_test.batch(BATCH_SIZE)\r\n",
        "  N_EPOCHS = 5\r\n",
        "\r\n",
        "  optimizer = optimizers.Adam(learning_rate=3e-5)\r\n",
        "  loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\r\n",
        "  model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy',f1_m,precision_m, recall_m])\r\n",
        "\r\n",
        "  model.fit(tfdataset_train,validation_data=tfdataset_test,\r\n",
        "            batch_size=64, epochs=N_EPOCHS)\r\n",
        "  print(\"evaluation\")\r\n",
        "  model.evaluate(tfdataset_test, return_dict=True, batch_size=BATCH_SIZE)\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Invoking lower casing\n",
            "Invoking lower casing\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ta_0gHJID5n"
      },
      "source": [
        "#X= hstack((df_train.question1.values,df_train['question2'])).toarray()\r\n",
        "#X=vectorizeTFIDF(df_train,'question1','question2',True,None,0)\r\n",
        "X=list('[CLS] '+df_train.question1+' [SEP] '+df_train.question2+' [SEP] ')\r\n",
        "y=df_train['is_duplicate']\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j3FkyQoihdUo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ef5b725-5407-4674-92c2-a0782f0ccfb7"
      },
      "source": [
        "#df_train.head(10)\r\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, stratify=y)\r\n",
        "runTransfomerModel('distil',300,X_train,y_train)\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertForSequenceClassification: ['vocab_projector', 'vocab_layer_norm', 'activation_13', 'vocab_transform']\n",
            "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier', 'dropout_79', 'classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-xwcJNfI0_FM"
      },
      "source": [
        "#df_train.head(10)\r\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.75, stratify=y)\r\n",
        "runTransfomerModel('albert',300,X_train,y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wnyz4smgj7Lg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7a16975-8940-4927-a4ec-21ed580dba27"
      },
      "source": [
        "\r\n",
        "MODEL_NAME = 'distilbert-base-uncased'\r\n",
        "tkzr = DistilBertTokenizer.from_pretrained(MODEL_NAME)\r\n",
        "model = TFDistilBertForSequenceClassification.from_pretrained(MODEL_NAME)\r\n",
        "MAX_LEN = 100\r\n",
        "y_train = list(map(int, y_train)) \r\n",
        "encodings = tkzr(X_train, max_length=100, truncation=True, padding=True)\r\n",
        "tfdataset = tf.data.Dataset.from_tensor_slices((dict(encodings),y_train))\r\n",
        "TEST_SPLIT = 0.50\r\n",
        "BATCH_SIZE = 16\r\n",
        "train_size = int(len(X_train) * (1-TEST_SPLIT))\r\n",
        "tfdataset = tfdataset.shuffle(len(X_train))\r\n",
        "tfdataset_train = tfdataset.take(train_size)\r\n",
        "tfdataset_test = tfdataset.skip(train_size)\r\n",
        "tfdataset_train = tfdataset_train.batch(BATCH_SIZE)\r\n",
        "tfdataset_test = tfdataset_test.batch(BATCH_SIZE)\r\n",
        "N_EPOCHS = 1\r\n",
        "\r\n",
        "optimizer = optimizers.Adam(learning_rate=3e-5, epsilon=1e-08, clipnorm=1.0)\r\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\r\n",
        "model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy',f1_m,precision_m, recall_m])\r\n",
        "\r\n",
        "model.fit(tfdataset_train,validation_data=tfdataset_test,\r\n",
        "          batch_size=16, epochs=N_EPOCHS)\r\n",
        "print(\"evaluation\")\r\n",
        "#model.evaluate(tfdataset_test, return_dict=True, batch_size=BATCH_SIZE)\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertForSequenceClassification: ['vocab_transform', 'vocab_layer_norm', 'activation_13', 'vocab_projector']\n",
            "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['dropout_109', 'classifier', 'pre_classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1264/1264 [==============================] - ETA: 0s - loss: 0.5089 - accuracy: 0.7361 - f1_m: 0.2670 - precision_m: 0.2303 - recall_m: 0.3865"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1264/1264 [==============================] - 322s 247ms/step - loss: 0.5088 - accuracy: 0.7361 - f1_m: 0.2671 - precision_m: 0.2304 - recall_m: 0.3866 - val_loss: 0.3386 - val_accuracy: 0.8485 - val_f1_m: 0.4766 - val_precision_m: 0.3617 - val_recall_m: 0.7639\n",
            "evaluation\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NWooPkHanhOI"
      },
      "source": [
        "y_pred=model.predict(tfdataset_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gORa6B3CpU79"
      },
      "source": [
        "preds = [np.argmax(i) for i in model.predict(tfdataset_test)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VC-CnvwklAAl"
      },
      "source": [
        "def runTransfomerModel(modelName,maxLen,X_train,y_train,X_test,y_test):\r\n",
        "  if(modelName=='distil'):\r\n",
        "    MODEL_NAME = 'distilbert-base-uncased'\r\n",
        "    tkzr = DistilBertTokenizer.from_pretrained(MODEL_NAME)\r\n",
        "    model = TFDistilBertForSequenceClassification.from_pretrained(MODEL_NAME)\r\n",
        "  if(modelName=='albert'):\r\n",
        "    MODEL_NAME = 'albert-base-v2'\r\n",
        "    tkzr = AlbertTokenizer.from_pretrained(MODEL_NAME)\r\n",
        "    model = TFAlbertForSequenceClassification.from_pretrained(MODEL_NAME)\r\n",
        "  MAX_LEN = maxLen\r\n",
        "  y_train = list(map(int, y_train))\r\n",
        "  y_test = list(map(int, y_test)) \r\n",
        "  encodings_train = tkzr(X_train, max_length=100, truncation=True, padding=True)\r\n",
        "  tfdataset_train = tf.data.Dataset.from_tensor_slices((dict(encodings_train),y_train))\r\n",
        "  encodings_test = tkzr(X_test, max_length=100, truncation=True, padding=True)\r\n",
        "  tfdataset_test = tf.data.Dataset.from_tensor_slices((dict(encodings_test),y_test))\r\n",
        "  TEST_SPLIT = 0.50\r\n",
        "  BATCH_SIZE = 16\r\n",
        "  # train_size = int(len(X_train) * (1-TEST_SPLIT))\r\n",
        "  # tfdataset = tfdataset.shuffle(len(X_train))\r\n",
        "  # tfdataset_train = tfdataset.take(train_size)\r\n",
        "  # tfdataset_test = tfdataset.skip(train_size)\r\n",
        "  #tfdataset_train, tfdataset_test = train_test_split(tfdataset, test_size=0.50, stratify=y)\r\n",
        "  tfdataset_train = tfdataset_train.batch(BATCH_SIZE)\r\n",
        "  tfdataset_test = tfdataset_test.batch(BATCH_SIZE)\r\n",
        "  N_EPOCHS = 10\r\n",
        "\r\n",
        "  optimizer = optimizers.Adam(learning_rate=3e-5, epsilon=1e-08, clipnorm=1.0)\r\n",
        "  loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\r\n",
        "  model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy',f1_m,precision_m, recall_m])\r\n",
        "\r\n",
        "  model.fit(tfdataset_train,validation_data=tfdataset_test,\r\n",
        "            batch_size=16, epochs=N_EPOCHS)\r\n",
        "  print(\"evaluation\")\r\n",
        "  model.evaluate(tfdataset_test, return_dict=True, batch_size=BATCH_SIZE)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 757,
          "referenced_widgets": [
            "8b6ce4e4df444b0b9547c28303423f62",
            "4085095406654de694e30e9f5160e275",
            "c6227c6f24a348a3bacedbe54c4ec5a8",
            "6232ee2c52a44a39884ca187c1623b00",
            "0cfbae2575ba4af991d2513f57309efe",
            "544c022dcfe941669d22634868fc6c7e",
            "9875233a7cca478599e1fec463d9e3ad",
            "fc39b58307584e04b608585a2dd9bf94",
            "7fe3007c909a48319f42c905b5a22cba",
            "ad6abdc74c574edd8d8cae9aea3d2a43",
            "655edcd3b9e041b78957c0a8b6ba619d",
            "69f0084e1dc14456a137b9a8b0cc5715",
            "983ac755dd774d70982e655363f2fc57",
            "89ab82506ee24a3090324e2689a5c0bd",
            "5a6fd32df45644358e5abb51f0f32b11",
            "f5955935bfab48f4a7485ae103b0c678",
            "6a76d12b5c7742bea23e65957de743df",
            "893a5f08d6cb4942910bf9dbd2e0cbc0",
            "2f8ebf45931b4df5a37637f895810afb",
            "9156b82b81b74e86b3a9b2d5a4659b16",
            "ba2e15ba7e7e4cb8b71172bf1a4bbbe9",
            "d8b0e76a93e3499fb10562492635d99a",
            "69602240370b45d4b4beb733391111a2",
            "b5670ce131c241d29e4b6074c18b7284"
          ]
        },
        "id": "Ip_3ME2hnWkU",
        "outputId": "805b7d18-70bc-4672-cb7d-3ddde5351257"
      },
      "source": [
        "runTransfomerModel('distil',100,X_train,y_train,X_test,y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8b6ce4e4df444b0b9547c28303423f62",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7fe3007c909a48319f42c905b5a22cba",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=442.0, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6a76d12b5c7742bea23e65957de743df",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=363423424.0, style=ProgressStyle(descri…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertForSequenceClassification: ['vocab_layer_norm', 'vocab_projector', 'vocab_transform', 'activation_13']\n",
            "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['dropout_19', 'pre_classifier', 'classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "WARNING:tensorflow:AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7f63786a9660>> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: <cyfunction Socket.send at 0x7f638fef1e58> is not a module, class, method, function, traceback, frame, or code object\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING: AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7f63786a9660>> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: <cyfunction Socket.send at 0x7f638fef1e58> is not a module, class, method, function, traceback, frame, or code object\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:AutoGraph could not transform <function wrap at 0x7f638d8868c8> and will run it as-is.\n",
            "Cause: while/else statement not yet supported\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING: AutoGraph could not transform <function wrap at 0x7f638d8868c8> and will run it as-is.\n",
            "Cause: while/else statement not yet supported\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2527/2527 [==============================] - ETA: 0s - loss: 0.4676 - accuracy: 0.7643 - f1_m: 0.2774 - precision_m: 0.2316 - recall_m: 0.3956"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r2527/2527 [==============================] - 1806s 702ms/step - loss: 0.4676 - accuracy: 0.7643 - f1_m: 0.2774 - precision_m: 0.2317 - recall_m: 0.3956 - val_loss: 0.3628 - val_accuracy: 0.8342 - val_f1_m: 0.4586 - val_precision_m: 0.3493 - val_recall_m: 0.7295\n",
            "Epoch 2/10\n",
            "2527/2527 [==============================] - 1777s 704ms/step - loss: 0.2938 - accuracy: 0.8704 - f1_m: 0.4433 - precision_m: 0.3309 - recall_m: 0.7437 - val_loss: 0.3738 - val_accuracy: 0.8455 - val_f1_m: 0.4860 - val_precision_m: 0.3559 - val_recall_m: 0.8333\n",
            "Epoch 3/10\n",
            "2527/2527 [==============================] - 1778s 704ms/step - loss: 0.1820 - accuracy: 0.9297 - f1_m: 0.4894 - precision_m: 0.3517 - recall_m: 0.8784 - val_loss: 0.4456 - val_accuracy: 0.8430 - val_f1_m: 0.4985 - val_precision_m: 0.3596 - val_recall_m: 0.8814\n",
            "Epoch 4/10\n",
            "1501/2527 [================>.............] - ETA: 3:02 - loss: 0.1196 - accuracy: 0.9557 - f1_m: 0.5044 - precision_m: 0.3579 - recall_m: 0.9359"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u3dkX_cAWLcG",
        "outputId": "4ae3afb2-ee64-40e0-c3c0-cfc7c1e56ae0"
      },
      "source": [
        "X_train"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eHDqpBV8By7b",
        "outputId": "0fef7199-044e-4245-e11e-00f168e78efd"
      },
      "source": [
        "\r\n",
        "MODEL_NAME = 'distilbert-base-uncased'\r\n",
        "MAX_LEN = 100\r\n",
        "\r\n",
        "review = X_train[10]\r\n",
        "\r\n",
        "tkzr = DistilBertTokenizer.from_pretrained(MODEL_NAME)\r\n",
        "\r\n",
        "inputs = tkzr(review, max_length=MAX_LEN, truncation=True, padding=True)\r\n",
        "y_train = list(map(int, y_train)) \r\n",
        "y_test = list(map(int, y_test)) \r\n",
        "print(f'review: \\'{review}\\'')\r\n",
        "print(f'input ids: {inputs[\"input_ids\"]}')\r\n",
        "print(f'attention mask: {inputs[\"attention_mask\"]}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "review: '[CLS] method to find separation of slits using fresnel biprism? [SEP] what are some of the things technicians can tell about the durability and reliability of laptops and its components? [SEP] '\n",
            "input ids: [101, 101, 4118, 2000, 2424, 8745, 1997, 29199, 2478, 10424, 2229, 11877, 12170, 18098, 2964, 1029, 102, 2054, 2024, 2070, 1997, 1996, 2477, 20202, 2064, 2425, 2055, 1996, 4241, 2527, 8553, 1998, 15258, 1997, 12191, 2015, 1998, 2049, 6177, 1029, 102, 102]\n",
            "attention mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "fxgnA_rjh9pn",
        "outputId": "7b4049f5-da96-4b70-c6d9-9a22b88d2f99"
      },
      "source": [
        "encodings_train = tkzr(X_train, max_length=100, truncation=True, padding=True,add_special_tokens=True)\r\n",
        "tfdataset_train = tf.data.Dataset.from_tensor_slices((dict(encodings_train),y_train))\r\n",
        "X_train[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'[CLS] what is the step by step guide to invest in share market in india? [SEP] what is the step by step guide to invest in share market? [SEP] '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f2CQ4Kw1WpL0"
      },
      "source": [
        "encodings_test = tkzr(X_test, max_length=100, truncation=True, padding=True)\r\n",
        "tfdataset_test = tf.data.Dataset.from_tensor_slices((dict(encodings_test),y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dNaTklF-NNud"
      },
      "source": [
        "TEST_SPLIT = 0.2\r\n",
        "BATCH_SIZE = 100\r\n",
        "\r\n",
        "train_size = int(len(X_train) * (1-TEST_SPLIT))\r\n",
        "\r\n",
        "# tfdataset_train = tfdataset_train.shuffle(len(X_train))\r\n",
        "# tfdataset_test = tfdataset_test.shuffle(len(X_train))\r\n",
        "# tfdataset_train = tfdataset_train.take(train_size)\r\n",
        "# tfdataset_test = tfdataset_train.skip(train_size)\r\n",
        "\r\n",
        "tfdataset_train_1 = tfdataset_train.batch(BATCH_SIZE)\r\n",
        "tfdataset_test_1 = tfdataset_test.batch(BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NXBk6YdjeMgr",
        "outputId": "314aa122-beba-449c-a9e3-781243a92987"
      },
      "source": [
        "tfdataset_train_1.shuffle(len(X_train))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<ShuffleDataset shapes: ({input_ids: (None, 100), attention_mask: (None, 100)}, (None,)), types: ({input_ids: tf.int32, attention_mask: tf.int32}, tf.int32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GMckxeY-hMBV",
        "outputId": "cb49d793-8ed3-4dec-f083-ede23bf04759"
      },
      "source": [
        "tfdataset_train_1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BatchDataset shapes: ({input_ids: (None, 100), attention_mask: (None, 100)}, (None,)), types: ({input_ids: tf.int32, attention_mask: tf.int32}, tf.int32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4gob_5zQhaHT",
        "outputId": "7c978a58-c44e-4ffd-ca13-2d1f05df75e8"
      },
      "source": [
        "tfdataset_test_1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BatchDataset shapes: ({input_ids: (None, 100), attention_mask: (None, 100)}, (None,)), types: ({input_ids: tf.int32, attention_mask: tf.int32}, tf.int32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q_PBuJOsDDZP",
        "outputId": "1bc7691c-b1bd-4460-b284-622460694c46"
      },
      "source": [
        "N_EPOCHS = 10\r\n",
        "\r\n",
        "model = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased')\r\n",
        "optimizer = optimizers.Adam(learning_rate=3e-5, epsilon=1e-08, clipnorm=1.0)\r\n",
        "# loss = losses.BinaryCrossentropy(from_logits=False)\r\n",
        "# model.compile(optimizer=optimizer, loss=loss, metrics=['BinaryAccuracy'])\r\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\r\n",
        "model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\r\n",
        "\r\n",
        "model.fit(tfdataset_train_1,batch_size=100, epochs=N_EPOCHS)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertForSequenceClassification: ['vocab_projector', 'vocab_layer_norm', 'activation_13', 'vocab_transform']\n",
            "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier', 'pre_classifier', 'dropout_39']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "41/41 [==============================] - 49s 944ms/step - loss: 0.6437 - accuracy: 0.6173\n",
            "Epoch 2/10\n",
            "41/41 [==============================] - 40s 980ms/step - loss: 0.4770 - accuracy: 0.7670\n",
            "Epoch 3/10\n",
            "41/41 [==============================] - 40s 964ms/step - loss: 0.3886 - accuracy: 0.8143\n",
            "Epoch 4/10\n",
            "41/41 [==============================] - 40s 977ms/step - loss: 0.2982 - accuracy: 0.8677\n",
            "Epoch 5/10\n",
            "41/41 [==============================] - 40s 974ms/step - loss: 0.2546 - accuracy: 0.8904\n",
            "Epoch 6/10\n",
            "41/41 [==============================] - 40s 978ms/step - loss: 0.1705 - accuracy: 0.9396\n",
            "Epoch 7/10\n",
            "41/41 [==============================] - 40s 975ms/step - loss: 0.1662 - accuracy: 0.9374\n",
            "Epoch 8/10\n",
            "41/41 [==============================] - 40s 971ms/step - loss: 0.1410 - accuracy: 0.9421\n",
            "Epoch 9/10\n",
            "41/41 [==============================] - 40s 978ms/step - loss: 0.0690 - accuracy: 0.9751\n",
            "Epoch 10/10\n",
            "41/41 [==============================] - 40s 976ms/step - loss: 0.0427 - accuracy: 0.9855\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7efca5abd6a0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "02Lj8cK8fy8o",
        "outputId": "051330de-37bd-4a16-8243-50629df2ab6d"
      },
      "source": [
        "model.evaluate(tfdataset_test_1, return_dict=True, batch_size=100)\r\n",
        "tfdataset_test_1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BatchDataset shapes: ({input_ids: (None, 100), attention_mask: (None, 100)}, (None,)), types: ({input_ids: tf.int32, attention_mask: tf.int32}, tf.string)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-j6379A_vXfN"
      },
      "source": [
        "from pycontractions import Contractions\r\n",
        "\r\n",
        "# Load your favorite word2vec model\r\n",
        "cont = Contractions('GoogleNews-vectors-negative300.bin')\r\n",
        "\r\n",
        "# optional, prevents loading on first expand_texts call\r\n",
        "cont.load_models()\r\n",
        "\r\n",
        "out = list(cont.expand_texts([\"I'd like to know how I'd done that!\",\r\n",
        "                            \"We're going to the zoo and I don't think I'll be home for dinner.\",\r\n",
        "                            \"Theyre going to the zoo and she'll be home for dinner.\"], precise=True))\r\n",
        "print(out)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MKuby-YNUyDw"
      },
      "source": [
        "def runGRU(X,y,split,embeddingTime,inputMethod):\r\n",
        "  mlr,X,y,split,embeddingTime,inputs=MlModel(df_train.dropna(),'question1','question2','is_duplicate',True,'lem','tfidf',0,300,'overSample','lr')\r\n",
        "  X_1=X[:, :, None]\r\n",
        "  from keras import Sequential\r\n",
        "  from keras.layers import LSTM\r\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.75, stratify=y)\r\n",
        "\r\n",
        "  model = Sequential()\r\n",
        "  model.add(Embedding(2000, 300, input_length=300))\r\n",
        "  #model.add(Bidirectional(GRU(200, return_sequences=True,dropout=0.2, recurrent_dropout=0.2)))\r\n",
        "  # model.add(GRU(512))\r\n",
        "  # model.add(Dense(48, activation='tanh'))\r\n",
        "  # model.add(Dense(1, activation='sigmoid'))\r\n",
        "  model.add(Bidirectional(GRU(300,return_sequences=True)))\r\n",
        "  model.add(Bidirectional(GRU(300,return_sequences=True)))\r\n",
        "  model.add(Bidirectional(GRU(300,return_sequences=True)))\r\n",
        "  model.add(GlobalMaxPooling1D())\r\n",
        "  model.add(Dense(16,activation='relu'))\r\n",
        "  model.add(Dropout(0.1))\r\n",
        "  model.add(Dense(1, activation=\"sigmoid\"))\r\n",
        "  optimizer = optimizers.Adam(learning_rate=0.0001)\r\n",
        "  model.compile(loss='sparse_categorical_crossentropy', optimizer='Adam', metrics=['accuracy',f1_m,precision_m, recall_m])\r\n",
        "  print(model.summary())\r\n",
        "  file='/content/drive/MyDrive/LJMU Masters/code/'+\"\".join(inputMethod)+'.png'\r\n",
        "  plot_model(model, to_file=file, show_shapes=True, show_layer_names=True)\r\n",
        "\r\n",
        "  y_train = y_train.astype(np.float)\r\n",
        "  y_test = y_test.astype(np.float)\r\n",
        "  model.fit(X_train, y_train, epochs=3, batch_size=64)\r\n",
        "  start_predtrain_time = time()\r\n",
        "  y_pred_train = model.predict(X_train)\r\n",
        "  predTrainingTime=time() - start_predtrain_time\r\n",
        "  start_predtrain_time = time()\r\n",
        "  y_pred_test = model.predict(X_test)\r\n",
        "  predTestTime=time() - start_predtrain_time\r\n",
        "  #cross_val_score(mlr, X, y, cv=10, scoring='recall_macro')\r\n",
        "  lst=evalMetrics(y_train,y_pred_train,y_test,y_pred_test)\r\n",
        "  return lst\r\n",
        "\r\n",
        "def runGRUProcess(preProcess,vect,embeddingSize=100,split='N'):\r\n",
        "  mlr,X,y,split,embeddingTime,inputMethod=MlModel(df_train,'question1','question2','is_duplicate',True,preProcess,vect,0,embeddingSize,split,'gru')\r\n",
        "  mdl=runGRU(X,y,split,embeddingTime,inputMethod)  \r\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.50, stratify=y)\r\n",
        "  mdl.fit(X_train, y_train)    \r\n",
        "  timeLogger(\"END Time ML modelling\")\r\n",
        "  start_predtrain_time = time()\r\n",
        "  y_pred_train = mdl.predict(X_train)\r\n",
        "  predTrainingTime=time() - start_predtrain_time\r\n",
        "  start_predtrain_time = time()\r\n",
        "  y_pred_test = mdl.predict(X_test)\r\n",
        "  predTestTime=time() - start_predtrain_time\r\n",
        "  #cross_val_score(mlr, X, y, cv=10, scoring='recall_macro')\r\n",
        "  lst=evalMetrics(y_train,y_pred_train,y_test,y_pred_test)\r\n",
        "  lst.append(embeddingTime)\r\n",
        "  lst.append(trainingTime)\r\n",
        "  lst.append(predTrainingTime)\r\n",
        "  lst.append(predTestTime)\r\n",
        "  \r\n",
        "  pltConfusion(y_train, y_pred_train,'train',inputMethod)\r\n",
        "  pltConfusion(y_test, y_pred_test,'test',inputMethod)\r\n",
        "\r\n",
        "  return lst"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zkvl-7Ih7Bji"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}